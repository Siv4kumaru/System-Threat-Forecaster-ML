# System Threat Forecaster: Malware Prediction! ğŸ¦ ğŸ’»

Hey Everyone! ğŸ‘‹ Welcome to my project here I tackled the challenge of predicting whether a  computer might get infected with malware with a given data set. Think of it like a digital immune system check!

This work was part for IITM BS diploma Level Machine Learning Practice Course, of the System Threat Forecaster competition on Kaggle! You can check it out here:

**Competition Link:** [Kaggle/System-Threat-Forecaster](https://www.kaggle.com/competitions/System-Threat-Forecaster) ğŸ†

## The Mission ğŸ¯

The goal was simple (well, kinda! ğŸ˜‰): Use a bunch of system data (telemetry) to predict if a machine (`target` = 1) had malware detected on it. The data came from antivirus threat reports.

## The Data Stash ğŸ’¾ğŸ”

*   **Source:** System Threat Forecaster (Kaggle-like dataset)
*   **What's inside:** Lots of details about different computers, identified by `MachineID`.
*   **Files:**
    *   `train.csv`: My training playground, full of features and the all-important `target` (0 or 1).
    *   `test.csv`: The mystery machines! Features provided, but I had to predict the `target`.
    *   `sample_submission.csv`: The format for my final predictions.

## My Project Journey: Step-by-Step! ğŸ—ºï¸

Here's how I went about building the predictor:

1.  **Loading Up ğŸ”ƒ:** First things first, I loaded up the `train`, `test`, and `sample_submission` files using Pandas. I immediately separated the `target` variable from the training set â€“ gotta keep that safe!

2.  **Detective Work (EDA) ğŸ•µï¸â€â™‚ï¸ğŸ”¬:** Time to put on my detective hat!
    *   Visualized feature distributions (like `PlatformType`).
    *   Checked the target balance â€“ were there roughly equal numbers of infected (1) and clean (0) machines? Yep, pretty balanced! âœ…
    *   Hunted down those pesky missing values (nulls) ğŸ“Š and saw which features needed some TLC later.
    *   Looked at how things like the number of antivirus products installed related to getting infected.

3.  **Feature Slimdown Time! ğŸ’ªâœ‚ï¸:** Some features just weren't pulling their weight or were clones of others!
    *   Dropped features that were *super* similar (high correlation). ğŸ‘‹
    *   Dropped features that barely changed (low variance) â€“ yawn! ğŸ˜´ (Found 15 of these!).
    *   Got rid of `MachineID` â€“ it's just an ID, doesn't help predict!

4.  **Splitting the Crew ğŸª“:** To properly test my models later, I split my *training* data into a new training set (`Xtrain`) and a validation set (`Xtest`) using `train_test_split`.

5.  **The Preprocessing Makeover ğŸ¤–ğŸ”§âœ¨:** Data needs cleaning and standardizing! I built a `sklearn` Pipeline using `ColumnTransformer` to handle everything automatically:
    *   **Numbers ğŸ”¢:** Filled in missing values (`SimpleImputer`) and put them all on the same scale (`StandardScaler`).
    *   **Categories (Few Options) ğŸ·ï¸:** Filled missings (`most_frequent`) and turned them into binary columns (`OneHotEncoder`). *Poof!* More columns, but the models understand them better.
    *   **Categories (Lots of Options) ğŸ·ï¸:** Filled missings (`most_frequent`) and assigned numbers (`OrdinalEncoder`).
    *   **Dates ğŸ“…:** Created a cool custom function (`FunctionTransformer`) to pull out the month, day, year, and even the time difference between the OS install and the scan! Then filled any gaps.

6.  **The Model Gauntlet ğŸ¥Š:** Which model would reign supreme? I trained and tested several contenders on my preprocessed data:
    *   XGBoost â©
    *   Random Forest ğŸŒ³
    *   LightGBM âš¡ (Spoiler: This one looked promising!)
    *   SGD Classifier ğŸ—¿
    *   AdaBoost ğŸ’£
    *   Evaluated them using accuracy and detailed reports on the validation set. LightGBM came out on top! ğŸ†

7.  **Picking the Feature All-Stars ğŸ›ï¸:** With LightGBM crowned the champ, I used its feature importances (`SelectFromModel`) to pick the *most* valuable players. This trimmed the feature count way down (from 153 after OHE to 88) without hurting performance much! Efficiency! âœ¨

8.  **Tuning the Champion ğŸ› ï¸âš™ï¸:**
    *   Retrained LightGBM with just the selected features â€“ still performed great!
    *   Used `RandomizedSearchCV` to hunt for the *absolute best* settings (hyperparameters) for LightGBM.
    *   Trained the final, tuned-up LightGBM model on the selected training features.

9.  **The Grand Finale! ğŸ‰:** Time for the *real* test data!
    *   Applied the *entire fitted preprocessing pipeline* to the original `test.csv`.
    *   Applied the *fitted feature selector* to the processed test data.
    *   Used my final, tuned LightGBM model to make the predictions! ğŸ”®
    *   Saved the results into `submission.csv`.

## The Workflow Map! ğŸ—ºï¸
```mermaid
graph TD
    
    subgraph Data_Loading
        A[Start] --> B[Load CSVs: train.csv, test.csv, sample_submission.csv]
        B --> C[Separate Target Ytrain from train data]
    end

    subgraph EDA_Feature_Engineering
        C --> D["Exploratory Data Analysis (EDA)"]
        D --> E[Drop High Correlation Features]
        E --> F[Drop Low Variance Features]
        F --> G[Drop MachineID Column]
    end


    subgraph Train_Test_Split   
        G --> H[Split train data into Xtrain, Xtest, Ytrain_split, Ytest_split]
    end

    
    subgraph Preprocessing_Pipeline
        I[Define Preprocessing Steps: Impute, DateTransform, OHE, OrdinalEncode]
        J[Define Scaling: StandardScaler]
        K[Combine into cleanPipeline]
        I --> K
        J --> K
    end


    subgraph Apply_Preprocessing_Scaling    
        H --> L[Fit cleanPipeline on Xtrain]
        L --> M["CleanedData (Scaled Train Features)"]
        H --> N[Transform Xtest using fitted cleanPipeline]
        N --> O["cleanedtestData (Scaled Test Features)"]
    end

    subgraph Model_Selection
        M --> P[Train Models: XGBoost, RandomForest, LightGBM, SGD, AdaBoost]
        O --> P
        P --> Q[Evaluate Models & Select Best: LightGBM]
    end


    subgraph Feature_Selection
        M --> R[Apply Feature Selection using trained LGBM]
        R --> S[selectedXTrain Features]
        O --> R
        R --> T[selectedXTest Features]
    end


    subgraph Model_Retraining_Tuning
        S --> U[Retrain LGBM on Selected Features]
        T --> U
        U --> V[Evaluate Retrained LGBM]
        S --> W[Hyperparameter Tune LGBM - RandomizedSearchCV]
        W --> X[Get Best LGBM Parameters]
        X --> Y[Train Final LGBM with Best Params on selectedXTrain]
        T --> Y
        Y --> Z[Evaluate Final Tuned LGBM]
    end

    %% Final Prediction & Submission
    subgraph Final_Prediction_Submission
        AA[Load Original FINtest Data] --> BB[Apply cleanPipeline to FINtest]
        BB --> CC[Apply Feature Selector to Processed FINtest]
        CC --> DD[SelectedFinaltest]
        Z --> EE[Predict on SelectedFinaltest]
        DD --> EE
        EE --> FF[Create submission.csv]
        FF --> GG[End]
    end

    %% Connections Between Subgraphs
    C --> D
    G --> H
    H --> K
    O --> Q
    V --> W
    Z --> AA
```
The Results! ğŸ…

And the Winner Is... ğŸ†âš¡ LightGBM!

After all the steps â€“ cleaning, transforming, selecting, and tuning â€“ the final LightGBM model achieved an accuracy of about 64.86% on my validation set. Not bad for predicting pesky malware! ğŸ‰ğŸ¯
